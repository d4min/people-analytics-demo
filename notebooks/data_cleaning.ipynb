{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary to import db_connector script\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Add project root to sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import datetime \n",
    "import re \n",
    "\n",
    "from db_connector import load_from_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "data = load_from_excel()\n",
    "\n",
    "# Make copies to avoid modifying originals\n",
    "cleaned_data = {key: df.copy() for key, df in data.items()}\n",
    "\n",
    "print(\"Data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Requisitions Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cleaning Requisitions Dataset ===\n",
      "\n",
      "2. Handling missing values:\n",
      "   - Missing RECRUITER values: 0 (0.00%)\n",
      "   - Found 37 unique RECRUITER_ID to RECRUITER mappings\n",
      "   - Filled 0 missing RECRUITER values\n",
      "   - Remaining missing RECRUITER values: 0 (0.00%)\n",
      "   - Flagged 296 records with missing CLOSE_DATE\n",
      "\n",
      "3. Found 25 requisitions with more than 10 openings\n",
      "\n",
      "Requisitions cleaning completed.\n",
      "Original shape: (5025, 11), Cleaned shape: (5025, 12)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5025 entries, 0 to 5024\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   REQUISITION_ID      5025 non-null   int64         \n",
      " 1   REQUISITION_UID     5025 non-null   int64         \n",
      " 2   STATUS_IN           5025 non-null   object        \n",
      " 3   OPEN_DATE           5025 non-null   datetime64[ns]\n",
      " 4   CLOSE_DATE          4729 non-null   datetime64[ns]\n",
      " 5   NUMBER_OF_OPENINGS  5025 non-null   int64         \n",
      " 6   DEPARTMENT_ID       5025 non-null   int64         \n",
      " 7   DEPARTMENT_NAME     5025 non-null   object        \n",
      " 8   RECRUITER_ID        5025 non-null   int64         \n",
      " 9   RECRUITER           5025 non-null   object        \n",
      " 10  LAST_MODIFIED_DATE  5025 non-null   datetime64[ns]\n",
      " 11  MISSING_CLOSE_DATE  5025 non-null   bool          \n",
      "dtypes: bool(1), datetime64[ns](3), int64(5), object(3)\n",
      "memory usage: 436.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Cleaning Requisitions Table ===\")\n",
    "\n",
    "# 1. Handling missing values \n",
    "# a. Fix RECRUITER field (30.6% missing)\n",
    "\n",
    "print(\"\\n1. Handling missing values:\")\n",
    "missing_recruiter = cleaned_data['requisitions']['RECRUITER'].isnull().sum()\n",
    "print(f\"   - Missing RECRUITER values: {missing_recruiter} ({missing_recruiter/len(cleaned_data['requisitions'])*100:.2f}%)\")\n",
    "\n",
    "# Use RECRUITER_ID to fill in missing RECRUITER values where possible\n",
    "recruiter_map = cleaned_data['requisitions'][cleaned_data['requisitions']['RECRUITER'].notna()].groupby('RECRUITER_ID')['RECRUITER'].first().to_dict()\n",
    "print(f\"   - Found {len(recruiter_map)} unique RECRUITER_ID to RECRUITER mappings\")\n",
    "\n",
    "# Fill missing values using the mapping\n",
    "before_fill = cleaned_data['requisitions']['RECRUITER'].isnull().sum()\n",
    "cleaned_data['requisitions']['RECRUITER'] = cleaned_data['requisitions'].apply(\n",
    "    lambda row: recruiter_map.get(row['RECRUITER_ID']) if pd.isnull(row['RECRUITER']) and row['RECRUITER_ID'] in recruiter_map else row['RECRUITER'], \n",
    "    axis=1\n",
    ")\n",
    "after_fill = cleaned_data['requisitions']['RECRUITER'].isnull().sum()\n",
    "print(f\"   - Filled {before_fill - after_fill} missing RECRUITER values\")\n",
    "print(f\"   - Remaining missing RECRUITER values: {after_fill} ({after_fill/len(cleaned_data['requisitions'])*100:.2f}%)\")\n",
    "\n",
    "# b. Add flag for missing CLOSE_DATE (5.9% missing) - these are likely still open\n",
    "cleaned_data['requisitions']['MISSING_CLOSE_DATE'] = cleaned_data['requisitions']['CLOSE_DATE'].isnull()\n",
    "print(f\"   - Flagged {cleaned_data['requisitions']['MISSING_CLOSE_DATE'].sum()} records with missing CLOSE_DATE\")\n",
    "\n",
    "# 2. Check for unusual NUMBER_OF_OPENINGS\n",
    "high_openings = cleaned_data['requisitions'][cleaned_data['requisitions']['NUMBER_OF_OPENINGS'] > 10]\n",
    "print(f\"\\n2. Found {len(high_openings)} requisitions with more than 10 openings\")\n",
    "\n",
    "# 3. Create a cleaned version ready for analysis\n",
    "cleaned_requisitions = cleaned_data['requisitions'].copy()\n",
    "print(\"\\nRequisitions cleaning completed.\")\n",
    "print(f\"Original shape: {data['requisitions'].shape}, Cleaned shape: {cleaned_requisitions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Candidate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cleaning Candidate Dataset ===\n",
      "1. Found 0 duplicate records (0.00%)\n",
      "   - Removed 0 duplicate records\n",
      "   - Converted SUBMISSION_DATE to datetime format\n",
      "   - Converted CANDIDATE_ID to datetime format\n",
      "   - Converted CANDIDATE_HISTORICAL_STATUS to datetime format\n",
      "\n",
      "3. Found 5723 records where status start date is before submission date\n",
      "   - These will be flagged but kept in the dataset\n",
      "\n",
      "4. Found 17 records with missing CANDIDATE_ID\n",
      "   - These will be flagged but kept in the dataset\n",
      "\n",
      "Candidate cleaning completed.\n",
      "Original shape: (615707, 9), Cleaned shape: (615707, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3s/27y6xzfx5j55fg4s9swmhjsc0000gn/T/ipykernel_28993/4223145300.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  cleaned_data['candidate'][col] = pd.to_datetime(cleaned_data['candidate'][col], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Cleaning Candidate Dataset ===\")\n",
    "\n",
    "# 1. Handle duplicates\n",
    "duplicate_count = cleaned_data['candidate'].duplicated().sum()\n",
    "print(f\"1. Found {duplicate_count} duplicate records ({duplicate_count/len(cleaned_data['candidate'])*100:.2f}%)\")\n",
    "cleaned_data['candidate'] = cleaned_data['candidate'].drop_duplicates()\n",
    "print(f\"   - Removed {duplicate_count} duplicate records\")\n",
    "\n",
    "# 2. Fix date columns\n",
    "date_cols = [col for col in cleaned_data['candidate'].columns if 'DATE' in col]\n",
    "for col in date_cols:\n",
    "    # Convert to datetime, set errors to coerce to handle invalid dates\n",
    "    if cleaned_data['candidate'][col].dtype != 'datetime64[ns]':\n",
    "        cleaned_data['candidate'][col] = pd.to_datetime(cleaned_data['candidate'][col], errors='coerce')\n",
    "        print(f\"   - Converted {col} to datetime format\")\n",
    "\n",
    "# 3. Handle records with illogical date sequences\n",
    "# Check submission_date to historical_status_start_date\n",
    "illogical_dates = cleaned_data['candidate'][\n",
    "    (cleaned_data['candidate']['SUBMISSION_DATE'].notna()) & \n",
    "    (cleaned_data['candidate']['HISTORICAL_STATUS_START_DATE'].notna()) & \n",
    "    (cleaned_data['candidate']['HISTORICAL_STATUS_START_DATE'] < cleaned_data['candidate']['SUBMISSION_DATE'])\n",
    "]\n",
    "print(f\"\\n3. Found {len(illogical_dates)} records where status start date is before submission date\")\n",
    "print(\"   - These will be flagged but kept in the dataset\")\n",
    "cleaned_data['candidate']['ILLOGICAL_DATE_FLAG'] = False\n",
    "cleaned_data['candidate'].loc[illogical_dates.index, 'ILLOGICAL_DATE_FLAG'] = True\n",
    "\n",
    "# 4. Handle missing CANDIDATE_ID values\n",
    "missing_ids = cleaned_data['candidate'][cleaned_data['candidate']['CANDIDATE_ID'].isna()]\n",
    "print(f\"\\n4. Found {len(missing_ids)} records with missing CANDIDATE_ID\")\n",
    "print(\"   - These will be flagged but kept in the dataset\")\n",
    "cleaned_data['candidate']['MISSING_ID_FLAG'] = cleaned_data['candidate']['CANDIDATE_ID'].isna()\n",
    "\n",
    "# 5. Create a clean version ready for analysis\n",
    "cleaned_candidate = cleaned_data['candidate'].copy()\n",
    "print(\"\\nCandidate cleaning completed.\")\n",
    "print(f\"Original shape: {data['candidate'].shape}, Cleaned shape: {cleaned_candidate.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Candidate Status Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Found 0 duplicate records\n",
      "   - Removed 0 duplicate records\n",
      "\n",
      "Candidate Status cleaning completed.\n",
      "Original shape: (16, 2), Cleaned shape: (16, 2)\n"
     ]
    }
   ],
   "source": [
    "# 1. Remove duplicates\n",
    "duplicate_count = cleaned_data['candidate_status'].duplicated().sum()\n",
    "print(f\"\\n1. Found {duplicate_count} duplicate records\")\n",
    "cleaned_data['candidate_status'] = cleaned_data['candidate_status'].drop_duplicates()\n",
    "print(f\"   - Removed {duplicate_count} duplicate records\")\n",
    "\n",
    "# 3. Create a clean version ready for analysis\n",
    "cleaned_candidate_status = cleaned_data['candidate_status'].copy()\n",
    "print(\"\\nCandidate Status cleaning completed.\")\n",
    "print(f\"Original shape: {data['candidate_status'].shape}, Cleaned shape: {cleaned_candidate_status.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary of Cleaning Results ===\n",
      "requisitions: Original (5025, 11) -> Cleaned (5025, 12)\n",
      "candidate: Original (615707, 9) -> Cleaned (615707, 11)\n",
      "candidate_status: Original (16, 2) -> Cleaned (16, 2)\n",
      "department: Original (392, 4) -> Cleaned (392, 4)\n",
      "\n",
      "Cleaning process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned data to cleaned_data dictionary\n",
    "cleaned_data = {\n",
    "    'requisitions': cleaned_requisitions,\n",
    "    'candidate': cleaned_candidate,\n",
    "    'candidate_status': cleaned_candidate_status,\n",
    "    'department': cleaned_data['department']\n",
    "}\n",
    "\n",
    "print(\"\\n=== Summary of Cleaning Results ===\")\n",
    "for table_name, df in cleaned_data.items():\n",
    "    original_shape = data[table_name].shape\n",
    "    cleaned_shape = df.shape\n",
    "    print(f\"{table_name}: Original {original_shape} -> Cleaned {cleaned_shape}\")\n",
    "\n",
    "print(\"\\nCleaning process completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aswatson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
