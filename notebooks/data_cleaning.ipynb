{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary to import db_connector script\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Add project root to sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import datetime \n",
    "import re \n",
    "\n",
    "from db_connector import load_from_excel, get_sqlalchemy_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "data = load_from_excel()\n",
    "\n",
    "# Make copies to avoid modifying originals\n",
    "cleaned_data = {key: df.copy() for key, df in data.items()}\n",
    "\n",
    "print(\"Data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Requisitions Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cleaning Requisitions Table ===\n",
      "\n",
      "1. Handling missing values:\n",
      "   - Missing RECRUITER values: 1540 (30.65%)\n",
      "   - Found 37 unique RECRUITER_ID to RECRUITER mappings\n",
      "   - Filled 1540 missing RECRUITER values\n",
      "   - Remaining missing RECRUITER values: 0 (0.00%)\n",
      "   - Found 296 requisitions with missing CLOSE_DATE (likely still open)\n",
      "   - 296 of these are correctly marked as 'Open' in STATUS_IN\n",
      "\n",
      "2. Found 25 requisitions with more than 10 openings\n",
      "\n",
      "3. Found 0 requisitions with close date before open date\n",
      "   - These have been flagged with INVALID_DATES_FLAG\n",
      "   - Added time_to_fill calculation and flagged 197 outliers\n",
      "\n",
      "Requisitions cleaning completed.\n",
      "Original shape: (5025, 11), Cleaned shape: (5025, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Cleaning Requisitions Table ===\")\n",
    "\n",
    "# 1. Handling missing values \n",
    "# a. Fix RECRUITER field (30.6% missing)\n",
    "\n",
    "print(\"\\n1. Handling missing values:\")\n",
    "missing_recruiter = cleaned_data['requisitions']['RECRUITER'].isnull().sum()\n",
    "print(f\"   - Missing RECRUITER values: {missing_recruiter} ({missing_recruiter/len(cleaned_data['requisitions'])*100:.2f}%)\")\n",
    "\n",
    "# Use RECRUITER_ID to fill in missing RECRUITER values where possible\n",
    "recruiter_map = cleaned_data['requisitions'][cleaned_data['requisitions']['RECRUITER'].notna()].groupby('RECRUITER_ID')['RECRUITER'].first().to_dict()\n",
    "print(f\"   - Found {len(recruiter_map)} unique RECRUITER_ID to RECRUITER mappings\")\n",
    "\n",
    "# Fill missing values using the mapping\n",
    "before_fill = cleaned_data['requisitions']['RECRUITER'].isnull().sum()\n",
    "cleaned_data['requisitions']['RECRUITER'] = cleaned_data['requisitions'].apply(\n",
    "    lambda row: recruiter_map.get(row['RECRUITER_ID']) if pd.isnull(row['RECRUITER']) and row['RECRUITER_ID'] in recruiter_map else row['RECRUITER'], \n",
    "    axis=1\n",
    ")\n",
    "after_fill = cleaned_data['requisitions']['RECRUITER'].isnull().sum()\n",
    "print(f\"   - Filled {before_fill - after_fill} missing RECRUITER values\")\n",
    "print(f\"   - Remaining missing RECRUITER values: {after_fill} ({after_fill/len(cleaned_data['requisitions'])*100:.2f}%)\")\n",
    "\n",
    "# b.  Handle missing CLOSE_DATE (5.9% missing) - these are likely still open\n",
    "open_reqs = cleaned_data['requisitions'][cleaned_data['requisitions']['CLOSE_DATE'].isnull()]\n",
    "print(f\"   - Found {len(open_reqs)} requisitions with missing CLOSE_DATE (likely still open)\")\n",
    "\n",
    "# Check if these align with STATUS_IN\n",
    "if 'STATUS_IN' in cleaned_data['requisitions'].columns:\n",
    "    still_open = open_reqs[open_reqs['STATUS_IN'] == 'Open']\n",
    "    incorrectly_marked = open_reqs[open_reqs['STATUS_IN'] != 'Open']\n",
    "    \n",
    "    print(f\"   - {len(still_open)} of these are correctly marked as 'Open' in STATUS_IN\")\n",
    "    if len(incorrectly_marked) > 0:\n",
    "        print(f\"   - {len(incorrectly_marked)} have missing CLOSE_DATE but aren't marked as 'Open'\")\n",
    "\n",
    "        cleaned_data['requisitions'].loc[incorrectly_marked.index, 'STATUS_IN'] = 'Open'\n",
    "\n",
    "# 2. Check for unusual NUMBER_OF_OPENINGS\n",
    "high_openings = cleaned_data['requisitions'][cleaned_data['requisitions']['NUMBER_OF_OPENINGS'] > 10]\n",
    "print(f\"\\n2. Found {len(high_openings)} requisitions with more than 10 openings\")\n",
    "\n",
    "# 3. Handle invalid dates and time-to-fill outliers\n",
    "# a. Flag requisitions with close date before open date\n",
    "invalid_dates = cleaned_data['requisitions'][\n",
    "    cleaned_data['requisitions']['CLOSE_DATE'].notna() & \n",
    "    (cleaned_data['requisitions']['CLOSE_DATE'] < cleaned_data['requisitions']['OPEN_DATE'])\n",
    "]\n",
    "cleaned_data['requisitions']['INVALID_DATES_FLAG'] = False\n",
    "cleaned_data['requisitions'].loc[invalid_dates.index, 'INVALID_DATES_FLAG'] = True\n",
    "print(f\"\\n3. Found {len(invalid_dates)} requisitions with close date before open date\")\n",
    "print(\"   - These have been flagged with INVALID_DATES_FLAG\")\n",
    "\n",
    "# b. Calculate and flag time-to-fill outliers\n",
    "closed_reqs = cleaned_data['requisitions'][cleaned_data['requisitions']['CLOSE_DATE'].notna()].copy()\n",
    "if not closed_reqs.empty:\n",
    "    closed_reqs['time_to_fill'] = (closed_reqs['CLOSE_DATE'] - closed_reqs['OPEN_DATE']).dt.days\n",
    "    \n",
    "    # Calculate outlier bounds\n",
    "    q1 = closed_reqs['time_to_fill'].quantile(0.25)\n",
    "    q3 = closed_reqs['time_to_fill'].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = max(0, q1 - 1.5 * iqr)\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Add time_to_fill and outlier flag columns\n",
    "    cleaned_data['requisitions']['time_to_fill'] = closed_reqs['time_to_fill']\n",
    "    cleaned_data['requisitions']['TIME_TO_FILL_OUTLIER'] = False\n",
    "    cleaned_data['requisitions'].loc[\n",
    "        closed_reqs[\n",
    "            (closed_reqs['time_to_fill'] < lower_bound) | \n",
    "            (closed_reqs['time_to_fill'] > upper_bound)\n",
    "        ].index, \n",
    "        'TIME_TO_FILL_OUTLIER'\n",
    "    ] = True\n",
    "    \n",
    "    print(f\"   - Added time_to_fill calculation and flagged {cleaned_data['requisitions']['TIME_TO_FILL_OUTLIER'].sum()} outliers\") \n",
    "\n",
    "# 4. Create a cleaned version ready for analysis\n",
    "cleaned_requisitions = cleaned_data['requisitions'].copy()\n",
    "print(\"\\nRequisitions cleaning completed.\")\n",
    "print(f\"Original shape: {data['requisitions'].shape}, Cleaned shape: {cleaned_requisitions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Candidate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cleaning Candidate Dataset ===\n",
      "1. Found 0 duplicate records (0.00%)\n",
      "   - Removed 0 duplicate records\n",
      "\n",
      "2. Found 5723 records where status start date is before submission date\n",
      "   - These will be flagged but kept in the dataset\n",
      "\n",
      "3. Found 17 records with missing CANDIDATE_ID\n",
      "   - These will be flagged but kept in the dataset\n",
      "\n",
      "4. Found 73245 records with invalid status values\n",
      "   - These have been flagged with INVALID_STATUS_FLAG\n",
      "\n",
      "5. Pipeline timing outliers:\n",
      "   - Flagged 33354 timing outliers\n",
      "   - Normal range is 0 to 60.0 days\n",
      "\n",
      "5. Found 29 candidates with invalid status progressions\n",
      "   - These have been flagged with INVALID_PROGRESSION_FLAG\n",
      "\n",
      "Candidate cleaning completed.\n",
      "Original shape: (615707, 9), Cleaned shape: (615707, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3s/27y6xzfx5j55fg4s9swmhjsc0000gn/T/ipykernel_30847/2298357541.py:118: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  progression_results = cleaned_data['candidate'].groupby('CANDIDATE_ID').apply(check_status_progression)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Cleaning Candidate Dataset ===\")\n",
    "\n",
    "# 1. Handle duplicates\n",
    "duplicate_count = cleaned_data['candidate'].duplicated().sum()\n",
    "print(f\"1. Found {duplicate_count} duplicate records ({duplicate_count/len(cleaned_data['candidate'])*100:.2f}%)\")\n",
    "cleaned_data['candidate'] = cleaned_data['candidate'].drop_duplicates()\n",
    "print(f\"   - Removed {duplicate_count} duplicate records\")\n",
    "\n",
    "# 2. Fix date columns\n",
    "date_cols = ['SUBMISSION_DATE', 'HISTORICAL_STATUS_START_DATE', 'HISTORICAL_STATUS_END_DATE', 'LAST_MODIFIED_DATE']\n",
    "for col in date_cols:\n",
    "    # Convert to datetime, set errors to coerce to handle invalid dates\n",
    "    if cleaned_data['candidate'][col].dtype != 'datetime64[ns]':\n",
    "        cleaned_data['candidate'][col] = pd.to_datetime(cleaned_data['candidate'][col], errors='coerce')\n",
    "        print(f\"   - Converted {col} to datetime format\")\n",
    "\n",
    "# 3. Handle records with illogical date sequences\n",
    "# Check submission_date to historical_status_start_date\n",
    "illogical_dates = cleaned_data['candidate'][\n",
    "    (cleaned_data['candidate']['SUBMISSION_DATE'].notna()) & \n",
    "    (cleaned_data['candidate']['HISTORICAL_STATUS_START_DATE'].notna()) & \n",
    "    (cleaned_data['candidate']['HISTORICAL_STATUS_START_DATE'] < cleaned_data['candidate']['SUBMISSION_DATE'])\n",
    "]\n",
    "print(f\"\\n2. Found {len(illogical_dates)} records where status start date is before submission date\")\n",
    "print(\"   - These will be flagged but kept in the dataset\")\n",
    "cleaned_data['candidate']['ILLOGICAL_DATE_FLAG'] = False\n",
    "cleaned_data['candidate'].loc[illogical_dates.index, 'ILLOGICAL_DATE_FLAG'] = True\n",
    "\n",
    "# 3. Handle missing CANDIDATE_ID values\n",
    "missing_ids = cleaned_data['candidate'][cleaned_data['candidate']['CANDIDATE_ID'].isna()]\n",
    "print(f\"\\n3. Found {len(missing_ids)} records with missing CANDIDATE_ID\")\n",
    "print(\"   - These will be flagged but kept in the dataset\")\n",
    "cleaned_data['candidate']['MISSING_ID_FLAG'] = cleaned_data['candidate']['CANDIDATE_ID'].isna()\n",
    "\n",
    "# 4. Handle invalid status values\n",
    "if 'candidate_status' in data:\n",
    "    valid_statuses = set(data['candidate_status']['CANDIDATE_HISTORICAL_STATUS'])\n",
    "    invalid_statuses = cleaned_data['candidate'][~cleaned_data['candidate']['CANDIDATE_HISTORICAL_STATUS'].isin(valid_statuses)]\n",
    "    cleaned_data['candidate']['INVALID_STATUS_FLAG'] = ~cleaned_data['candidate']['CANDIDATE_HISTORICAL_STATUS'].isin(valid_statuses)\n",
    "    print(f\"\\n4. Found {len(invalid_statuses)} records with invalid status values\")\n",
    "    print(\"   - These have been flagged with INVALID_STATUS_FLAG\")\n",
    "\n",
    "# 5. Handle pipeline timing outliers\n",
    "# Calculate time between submission and status start\n",
    "timing_data = cleaned_data['candidate'][\n",
    "    (cleaned_data['candidate']['SUBMISSION_DATE'].notna()) & \n",
    "    (cleaned_data['candidate']['HISTORICAL_STATUS_START_DATE'].notna())\n",
    "].copy()\n",
    "\n",
    "if not timing_data.empty:\n",
    "    # Calculate days between submission and status start\n",
    "    timing_data['days_to_status'] = (\n",
    "        timing_data['HISTORICAL_STATUS_START_DATE'] - \n",
    "        timing_data['SUBMISSION_DATE']\n",
    "    ).dt.days\n",
    "    \n",
    "    # Only keep positive values (where status start is after submission)\n",
    "    timing_data = timing_data[timing_data['days_to_status'] >= 0]\n",
    "    \n",
    "    # Calculate outlier bounds\n",
    "    q1 = timing_data['days_to_status'].quantile(0.25)\n",
    "    q3 = timing_data['days_to_status'].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Add timing and outlier flag to main dataframe\n",
    "    cleaned_data['candidate']['days_to_status'] = timing_data['days_to_status']\n",
    "    cleaned_data['candidate']['TIMING_OUTLIER'] = False\n",
    "    cleaned_data['candidate'].loc[\n",
    "        timing_data[timing_data['days_to_status'] > upper_bound].index,\n",
    "        'TIMING_OUTLIER'\n",
    "    ] = True\n",
    "    \n",
    "    print(\"\\n5. Pipeline timing outliers:\")\n",
    "    print(f\"   - Flagged {cleaned_data['candidate']['TIMING_OUTLIER'].sum()} timing outliers\")\n",
    "    print(f\"   - Normal range is 0 to {upper_bound:.1f} days\")\n",
    "    \n",
    "# 6. Check and flag status progression logic\n",
    "# Define expected status progression\n",
    "status_order = {\n",
    "    'Applied': 1,\n",
    "    'Screening': 2,\n",
    "    'Interview': 3,\n",
    "    'Offer': 4,\n",
    "    'Hired': 5,\n",
    "    'Rejected': -1,  # Can happen at any stage\n",
    "    'Withdrawn': -1  # Can happen at any stage\n",
    "}\n",
    "\n",
    "# Create a status progression check\n",
    "def check_status_progression(group):\n",
    "    # Sort by status start date\n",
    "    sorted_statuses = group.sort_values('HISTORICAL_STATUS_START_DATE')\n",
    "    \n",
    "    prev_stage = 0\n",
    "    valid_progression = True\n",
    "    \n",
    "    for status in sorted_statuses['CANDIDATE_HISTORICAL_STATUS']:\n",
    "        if status not in status_order:\n",
    "            continue\n",
    "            \n",
    "        current_stage = status_order[status]\n",
    "        \n",
    "        # Skip progression check for rejected/withdrawn\n",
    "        if current_stage == -1:\n",
    "            continue\n",
    "            \n",
    "        # Check if current stage is valid progression\n",
    "        if current_stage < prev_stage:\n",
    "            valid_progression = False\n",
    "            break\n",
    "            \n",
    "        prev_stage = current_stage\n",
    "    \n",
    "    return valid_progression\n",
    "\n",
    "# Group by candidate and check progression\n",
    "progression_results = cleaned_data['candidate'].groupby('CANDIDATE_ID').apply(check_status_progression)\n",
    "\n",
    "# Add flag for invalid progressions\n",
    "cleaned_data['candidate']['INVALID_PROGRESSION_FLAG'] = False\n",
    "for candidate_id, is_valid in progression_results.items():\n",
    "    if not is_valid:\n",
    "        cleaned_data['candidate'].loc[\n",
    "            cleaned_data['candidate']['CANDIDATE_ID'] == candidate_id, \n",
    "            'INVALID_PROGRESSION_FLAG'\n",
    "        ] = True\n",
    "\n",
    "invalid_count = cleaned_data['candidate']['INVALID_PROGRESSION_FLAG'].sum()\n",
    "print(f\"\\n5. Found {invalid_count} candidates with invalid status progressions\")\n",
    "print(\"   - These have been flagged with INVALID_PROGRESSION_FLAG\")\n",
    "\n",
    "\n",
    "# 7. Create a clean version ready for analysis\n",
    "cleaned_candidate = cleaned_data['candidate'].copy()\n",
    "print(\"\\nCandidate cleaning completed.\")\n",
    "print(f\"Original shape: {data['candidate'].shape}, Cleaned shape: {cleaned_candidate.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Candidate Status Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Found 0 duplicate records\n",
      "   - Removed 0 duplicate records\n",
      "\n",
      "Candidate Status cleaning completed.\n",
      "Original shape: (16, 2), Cleaned shape: (16, 2)\n"
     ]
    }
   ],
   "source": [
    "# 1. Remove duplicates\n",
    "duplicate_count = cleaned_data['candidate_status'].duplicated().sum()\n",
    "print(f\"\\n1. Found {duplicate_count} duplicate records\")\n",
    "cleaned_data['candidate_status'] = cleaned_data['candidate_status'].drop_duplicates()\n",
    "print(f\"   - Removed {duplicate_count} duplicate records\")\n",
    "\n",
    "# 3. Create a clean version ready for analysis\n",
    "cleaned_candidate_status = cleaned_data['candidate_status'].copy()\n",
    "print(\"\\nCandidate Status cleaning completed.\")\n",
    "print(f\"Original shape: {data['candidate_status'].shape}, Cleaned shape: {cleaned_candidate_status.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary of Cleaning Results ===\n",
      "requisitions: Original (5025, 11) -> Cleaned (5025, 14)\n",
      "candidate: Original (615707, 9) -> Cleaned (615707, 15)\n",
      "candidate_status: Original (16, 2) -> Cleaned (16, 2)\n",
      "department: Original (392, 4) -> Cleaned (392, 4)\n",
      "\n",
      "Cleaning process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned data to cleaned_data dictionary\n",
    "cleaned_data = {\n",
    "    'requisitions': cleaned_requisitions,\n",
    "    'candidate': cleaned_candidate,\n",
    "    'candidate_status': cleaned_candidate_status,\n",
    "    'department': cleaned_data['department']\n",
    "}\n",
    "\n",
    "print(\"\\n=== Summary of Cleaning Results ===\")\n",
    "for table_name, df in cleaned_data.items():\n",
    "    original_shape = data[table_name].shape\n",
    "    cleaned_shape = df.shape\n",
    "    print(f\"{table_name}: Original {original_shape} -> Cleaned {cleaned_shape}\")\n",
    "\n",
    "print(\"\\nCleaning process completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_db(cleaned_data):\n",
    "    engine = get_sqlalchemy_engine()\n",
    "    for table_name, df in cleaned_data.items():\n",
    "        df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "    engine.dispose()\n",
    "\n",
    "upload_to_db(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aswatson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
