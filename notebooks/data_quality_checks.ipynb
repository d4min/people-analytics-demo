{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Assessment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary to import db_connector script\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Add project root to sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "from db_connector import load_from_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_from_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines data quality assessment function:\n",
    "\n",
    "def assess_data_quality(df, table_name):\n",
    "    print(f\"\\n{'='*30} {table_name.upper()} DATA QUALITY {'='*30}\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Convert date columns to datetime at the beginning\n",
    "    date_cols = [col for col in df.columns if 'DATE' in col]\n",
    "    for col in date_cols:\n",
    "        if df[col].dtype != 'datetime64[ns]':\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            print(f\"Converted {col} to datetime\")\n",
    "\n",
    "    # 1. Check for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\n1. Duplicates: {duplicates} ({duplicates/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # 2. Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = df.isnull().sum() / len(df) * 100\n",
    "    missing_info = pd.DataFrame({\n",
    "        'Missing Values': missing,\n",
    "        'Percentage': missing_pct\n",
    "    })\n",
    "    print(\"\\n2. Missing Values:\")\n",
    "    display(missing_info[missing_info['Missing Values'] > 0].sort_values('Missing Values', ascending=False))\n",
    "    \n",
    "    # 3. Check data types\n",
    "    print(\"\\n3. Data Types:\")\n",
    "    display(pd.DataFrame(df.dtypes, columns=['Data Type']))\n",
    "    \n",
    "    # 4. Specific checks based on table\n",
    "    print(\"\\n4. Specific Quality Checks:\")\n",
    "\n",
    "    if table_name == 'requisitions':\n",
    "        # a. Check for requisitions with close date before open date\n",
    "        if 'OPEN_DATE' in df.columns and 'CLOSE_DATE' in df.columns:\n",
    "            invalid_dates = df[df['CLOSE_DATE'].notna() & (df['CLOSE_DATE'] < df['OPEN_DATE'])]\n",
    "            print(f\"   - Requisitions with close date before open date: {len(invalid_dates)}\")\n",
    "            if len(invalid_dates) > 0:\n",
    "                display(invalid_dates.head())\n",
    "        \n",
    "        # b. Check for unusual number of openings\n",
    "        if 'NUMBER_OF_OPENINGS' in df.columns:\n",
    "            unusual_openings = df[df['NUMBER_OF_OPENINGS'] > 10]  \n",
    "            print(f\"   - Requisitions with more than 10 openings: {len(unusual_openings)}\")\n",
    "            if len(unusual_openings) > 0:\n",
    "                value_counts = df['NUMBER_OF_OPENINGS'].value_counts().sort_index()\n",
    "                display(value_counts)\n",
    "\n",
    "            # outlier detection for NUMBER_OF_OPENINGS\n",
    "            print(\"\\n   - Outlier detection for NUMBER_OF_OPENINGS:\")\n",
    "            q1 = df['NUMBER_OF_OPENINGS'].quantile(0.25)\n",
    "            q3 = df['NUMBER_OF_OPENINGS'].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            \n",
    "            outliers = df[df['NUMBER_OF_OPENINGS'] > upper_bound]\n",
    "            print(f\"     Outliers (>{upper_bound:.1f} openings): {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "            \n",
    "        # c. Check for time-to-fill outliers \n",
    "        if 'OPEN_DATE' in df.columns and 'CLOSE_DATE' in df.columns:\n",
    "            # Convert to datetime if needed\n",
    "            if df['OPEN_DATE'].dtype != 'datetime64[ns]':\n",
    "                df['OPEN_DATE'] = pd.to_datetime(df['OPEN_DATE'], errors='coerce')\n",
    "            if df['CLOSE_DATE'].dtype != 'datetime64[ns]':\n",
    "                df['CLOSE_DATE'] = pd.to_datetime(df['CLOSE_DATE'], errors='coerce')\n",
    "                \n",
    "            # Calculate time to fill for closed requisitions\n",
    "            closed_reqs = df[df['CLOSE_DATE'].notna()].copy()\n",
    "            if not closed_reqs.empty:\n",
    "                closed_reqs['time_to_fill'] = (closed_reqs['CLOSE_DATE'] - closed_reqs['OPEN_DATE']).dt.days\n",
    "                \n",
    "                # Check for outliers\n",
    "                print(\"\\n   - Outlier detection for time-to-fill:\")\n",
    "                q1 = closed_reqs['time_to_fill'].quantile(0.25)\n",
    "                q3 = closed_reqs['time_to_fill'].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                lower_bound = max(0, q1 - 1.5 * iqr)  # Can't be negative\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "                \n",
    "                lower_outliers = closed_reqs[closed_reqs['time_to_fill'] < lower_bound]\n",
    "                upper_outliers = closed_reqs[closed_reqs['time_to_fill'] > upper_bound]\n",
    "                \n",
    "                print(f\"     Lower outliers (<{lower_bound:.1f} days): {len(lower_outliers)} ({len(lower_outliers)/len(closed_reqs)*100:.2f}%)\")\n",
    "                print(f\"     Upper outliers (>{upper_bound:.1f} days): {len(upper_outliers)} ({len(upper_outliers)/len(closed_reqs)*100:.2f}%)\")\n",
    "                \n",
    "                print(\"\\n     Time-to-fill statistics (days):\")\n",
    "                display(closed_reqs['time_to_fill'].describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95]))\n",
    "\n",
    "    elif table_name == 'candidate':\n",
    "        # a. Check for candidates with status dates out of order\n",
    "        date_cols = [col for col in df.columns if 'DATE' in col and col != 'LAST_MODIFIED_DATE']\n",
    "        for i in range(len(date_cols)-1):\n",
    "            for j in range(i+1, len(date_cols)):\n",
    "                col1, col2 = date_cols[i], date_cols[j]\n",
    "                invalid_dates = df[(df[col1].notna()) & (df[col2].notna()) & (df[col2] < df[col1])]\n",
    "                if len(invalid_dates) > 0:\n",
    "                    print(f\"   - Records with {col2} before {col1}: {len(invalid_dates)}\")\n",
    "        \n",
    "        # b. Check for missing candidate IDs\n",
    "        if 'CANDIDATE_ID' in df.columns:\n",
    "            missing_ids = df[df['CANDIDATE_ID'].isna()]\n",
    "            print(f\"   - Records with missing candidate IDs: {len(missing_ids)}\")\n",
    "            \n",
    "        # c. Check for invalid statuses\n",
    "        if 'CANDIDATE_HISTORICAL_STATUS' in df.columns and 'candidate_status' in data:\n",
    "            valid_statuses = set(data['candidate_status']['CANDIDATE_HISTORICAL_STATUS'])\n",
    "            invalid_statuses = df[~df['CANDIDATE_HISTORICAL_STATUS'].isin(valid_statuses)]\n",
    "            print(f\"   - Records with invalid status values: {len(invalid_statuses)}\")\n",
    "            if len(invalid_statuses) > 0:\n",
    "                display(invalid_statuses['CANDIDATE_HISTORICAL_STATUS'].value_counts())\n",
    "\n",
    "        # d. Add outlier detection for submission-to-interview time \n",
    "        if 'SUBMISSION_DATE' in df.columns and 'INTERVIEW_DATE' in df.columns:\n",
    "\n",
    "            if df['SUBMISSION_DATE'].dtype != 'datetime64[ns]':\n",
    "                df['SUBMISSION_DATE'] = pd.to_datetime(df['SUBMISSION_DATE'], errors='coerce')\n",
    "            if df['INTERVIEW_DATE'].dtype != 'datetime64[ns]':\n",
    "                df['INTERVIEW_DATE'] = pd.to_datetime(df['INTERVIEW_DATE'], errors='coerce')\n",
    "            \n",
    "            sub_to_int = df[(df['SUBMISSION_DATE'].notna()) & (df['INTERVIEW_DATE'].notna())].copy()\n",
    "            if not sub_to_int.empty:\n",
    "                sub_to_int['days_to_interview'] = (sub_to_int['INTERVIEW_DATE'] - sub_to_int['SUBMISSION_DATE']).dt.days\n",
    "                sub_to_int = sub_to_int[sub_to_int['days_to_interview'] >= 0]  # Filter out negative values (errors)\n",
    "                \n",
    "                print(\"\\n   - Outlier detection for submission-to-interview time:\")\n",
    "                q1 = sub_to_int['days_to_interview'].quantile(0.25)\n",
    "                q3 = sub_to_int['days_to_interview'].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "                \n",
    "                upper_outliers = sub_to_int[sub_to_int['days_to_interview'] > upper_bound]\n",
    "                print(f\"     Upper outliers (>{upper_bound:.1f} days): {len(upper_outliers)} ({len(upper_outliers)/len(sub_to_int)*100:.2f}%)\")\n",
    "                \n",
    "                print(\"\\n     Submission-to-interview time statistics (days):\")\n",
    "                display(sub_to_int['days_to_interview'].describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95]))\n",
    "\n",
    "        # e. Add outlier detection for interview-to-hire time \n",
    "        if 'INTERVIEW_DATE' in df.columns and 'HIRED_DATE' in df.columns:\n",
    "            # Convert to datetime if needed\n",
    "            if df['INTERVIEW_DATE'].dtype != 'datetime64[ns]':\n",
    "                df['INTERVIEW_DATE'] = pd.to_datetime(df['INTERVIEW_DATE'], errors='coerce')\n",
    "            if df['HIRED_DATE'].dtype != 'datetime64[ns]':\n",
    "                df['HIRED_DATE'] = pd.to_datetime(df['HIRED_DATE'], errors='coerce')\n",
    "            \n",
    "            int_to_hire = df[(df['INTERVIEW_DATE'].notna()) & (df['HIRED_DATE'].notna())].copy()\n",
    "            if not int_to_hire.empty:\n",
    "                int_to_hire['days_to_hire'] = (int_to_hire['HIRED_DATE'] - int_to_hire['INTERVIEW_DATE']).dt.days\n",
    "                int_to_hire = int_to_hire[int_to_hire['days_to_hire'] >= 0]  # Filter out negative values (errors)\n",
    "                \n",
    "                print(\"\\n   - Outlier detection for interview-to-hire time:\")\n",
    "                q1 = int_to_hire['days_to_hire'].quantile(0.25)\n",
    "                q3 = int_to_hire['days_to_hire'].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "                \n",
    "                upper_outliers = int_to_hire[int_to_hire['days_to_hire'] > upper_bound]\n",
    "                print(f\"     Upper outliers (>{upper_bound:.1f} days): {len(upper_outliers)} ({len(upper_outliers)/len(int_to_hire)*100:.2f}%)\")\n",
    "                \n",
    "                print(\"\\n     Interview-to-hire time statistics (days):\")\n",
    "                display(int_to_hire['days_to_hire'].describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95]))\n",
    "    \n",
    "    elif table_name == 'department':\n",
    "        # a. Check for departments that are their own parent\n",
    "        if 'DEPARTMENT_ID' in df.columns and 'PARENT_DEPARTMENT_ID' in df.columns:\n",
    "            self_parent = df[df['DEPARTMENT_ID'] == df['PARENT_DEPARTMENT_ID']]\n",
    "            print(f\"   - Departments that are their own parent: {len(self_parent)}\")\n",
    "            if len(self_parent) > 0:\n",
    "                display(self_parent.head())\n",
    "            \n",
    "        # b. Check for consistency in naming patterns\n",
    "        if 'DEPARTMENT_NAME' in df.columns:\n",
    "            missing_sd = df[~df['DEPARTMENT_NAME'].str.contains('- SD')]\n",
    "            print(f\"   - Departments without '- SD' in name: {len(missing_sd)}\")\n",
    "            if len(missing_sd) > 0:\n",
    "                display(missing_sd['DEPARTMENT_NAME'].head())\n",
    "                         \n",
    "    elif table_name == 'candidate_status':\n",
    "        # Check for duplicate status values\n",
    "        if 'CANDIDATE_HISTORICAL_STATUS' in df.columns:\n",
    "            dup_status = df['CANDIDATE_HISTORICAL_STATUS'].duplicated().sum()\n",
    "            print(f\"   - Duplicate status values: {dup_status}\")\n",
    "            \n",
    "        # Check for missing stage mappings\n",
    "        if 'CANDIDATE_STAGE' in df.columns:\n",
    "            missing_stage = df[df['CANDIDATE_STAGE'].isna()]\n",
    "            print(f\"   - Statuses without stage mapping: {len(missing_stage)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each pipeline should have a logical order of candidate statuses \n",
    "# e.g. New Submission -> In Review -> Interview -> Offer -> Hired\n",
    "# Or: New Submission -> In Review -> Rejected\n",
    "# NOT: Interview -> New Submission -> Rejected -> Offer\n",
    "\n",
    "def check_candidate_status_logic(df):\n",
    "    print(\"\\n\" + \"=\"*30 + \" CANDIDATE STATUS LOGIC CHECKS \" + \"=\"*30)\n",
    "\n",
    "    # First, ensure we have datetime for status dates\n",
    "    if 'HISTORICAL_STATUS_START_DATE' in df.columns:\n",
    "        if df['HISTORICAL_STATUS_START_DATE'].dtype != 'datetime64[ns]':\n",
    "            df['HISTORICAL_STATUS_START_DATE'] = pd.to_datetime(df['HISTORICAL_STATUS_START_DATE'], errors='coerce')\n",
    "\n",
    "    # Get active vs. closed requisitions\n",
    "    active_reqs = set(data['requisitions'][data['requisitions']['STATUS_IN'] == 'Open']['REQUISITION_ID'])\n",
    "    closed_reqs = set(data['requisitions'][data['requisitions']['STATUS_IN'] == 'Closed']['REQUISITION_ID'])    \n",
    "\n",
    "    print(f\"Active requisitions: {len(active_reqs)}\")\n",
    "    print(f\"Closed requisitions: {len(closed_reqs)}\")\n",
    "\n",
    "    # Group by requisition and candidate to analyse each pipeline\n",
    "    pipeline_groups = df.groupby(['REQUISITION_ID', 'CANDIDATE_ID'])\n",
    "\n",
    "    # Get the last status for each pipeline\n",
    "    last_statuses = pipeline_groups.apply(lambda g: g.sort_values('HISTORICAL_STATUS_START_DATE').iloc[-1])\n",
    "\n",
    "    # Add flag for active vs. closed requisitions\n",
    "    last_statuses['REQUISITION_STATUS'] = last_statuses['REQUISITION_ID'].apply(\n",
    "        lambda x: 'Active' if x in active_reqs else 'Closed' if x in closed_reqs else 'Unknown'\n",
    "    )\n",
    "\n",
    "    # Check distribution of final statuses\n",
    "    print(\"\\nDistribution of final candidate statuses:\")\n",
    "    final_status_counts = last_statuses['CANDIDATE_HISTORICAL_STATUS'].value_counts()\n",
    "    display(final_status_counts)\n",
    "\n",
    "    # Identify potentially problematic final statuses for CLOSED requisitions only\n",
    "    expected_final_statuses = ['Hired', 'Rejected', 'Closed']\n",
    "    closed_req_pipelines = last_statuses[last_statuses['REQUISITION_STATUS'] == 'Closed']\n",
    "    \n",
    "    unexpected_final = closed_req_pipelines[~closed_req_pipelines['CANDIDATE_HISTORICAL_STATUS'].isin(expected_final_statuses)]\n",
    "    \n",
    "    print(f\"\\nClosed requisition pipelines not ending with expected final status (Hired/Rejected/Closed): {len(unexpected_final)} ({len(unexpected_final)/len(closed_req_pipelines)*100:.2f}%)\")\n",
    "    \n",
    "    if len(unexpected_final) > 0:\n",
    "        print(\"\\nTop unusual final statuses for closed requisitions:\")\n",
    "        display(unexpected_final['CANDIDATE_HISTORICAL_STATUS'].value_counts().head(10))\n",
    "        \n",
    "        print(\"\\nSample of closed requisition pipelines with unusual final status:\")\n",
    "        display(unexpected_final.head())\n",
    "\n",
    "    # Check for logical progression in status\n",
    "    print(\"\\nChecking for status sequence anomalies...\")\n",
    "    \n",
    "    # Define a simplified expected progression\n",
    "    early_stages = ['New Submission', 'In Review']\n",
    "    mid_stages = ['Interview', 'First Interview', 'Second Interview', 'Final Interview']\n",
    "    late_stages = ['Offer', 'Hired', 'Rejected', 'Closed']\n",
    "    \n",
    "    # Function to check if a sequence has logical progression issues\n",
    "    def has_sequence_issue(group):\n",
    "        # Sort by status date\n",
    "        sequence = group.sort_values('HISTORICAL_STATUS_START_DATE')\n",
    "        \n",
    "        # Check if late stages come before early stages\n",
    "        has_issue = False\n",
    "        \n",
    "        # If any late stage exists before an early stage\n",
    "        for late_idx, row in sequence[sequence['CANDIDATE_HISTORICAL_STATUS'].isin(late_stages)].iterrows():\n",
    "            late_date = row['HISTORICAL_STATUS_START_DATE']\n",
    "            early_after = sequence[(sequence['HISTORICAL_STATUS_START_DATE'] > late_date) & \n",
    "                                  (sequence['CANDIDATE_HISTORICAL_STATUS'].isin(early_stages))]\n",
    "            if not early_after.empty:\n",
    "                has_issue = True\n",
    "                break\n",
    "                \n",
    "        return has_issue\n",
    "\n",
    "    # Apply check to each pipeline\n",
    "    problematic_pipelines = pipeline_groups.apply(has_sequence_issue)\n",
    "    problem_count = problematic_pipelines.sum()\n",
    "    \n",
    "    print(f\"\\nPipelines with illogical status sequences: {problem_count} ({problem_count/len(pipeline_groups)*100:.2f}%)\")\n",
    "    \n",
    "    if problem_count > 0:\n",
    "        print(\"\\nSample problematic pipelines:\")\n",
    "        sample_problems = problematic_pipelines[problematic_pipelines].index[:5]\n",
    "        \n",
    "        for req_id, cand_id in sample_problems:\n",
    "            print(f\"\\nRequisition {req_id}, Candidate {cand_id}:\")\n",
    "            seq = df[(df['REQUISITION_ID'] == req_id) & (df['CANDIDATE_ID'] == cand_id)]\n",
    "            display(seq[['CANDIDATE_HISTORICAL_STATUS', 'HISTORICAL_STATUS_START_DATE']].sort_values('HISTORICAL_STATUS_START_DATE'))\n",
    "    \n",
    "    return {\n",
    "        'final_status_counts': final_status_counts,\n",
    "        'unexpected_final_count_closed_reqs': len(unexpected_final),\n",
    "        'problematic_sequence_count': problem_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== REQUISITIONS DATA QUALITY ==============================\n",
      "\n",
      "1. Duplicates: 0 (0.00%)\n",
      "\n",
      "2. Missing Values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RECRUITER</th>\n",
       "      <td>1540</td>\n",
       "      <td>30.646766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLOSE_DATE</th>\n",
       "      <td>296</td>\n",
       "      <td>5.890547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Missing Values  Percentage\n",
       "RECRUITER             1540   30.646766\n",
       "CLOSE_DATE             296    5.890547"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Data Types:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>REQUISITION_ID</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REQUISITION_UID</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STATUS_IN</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OPEN_DATE</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLOSE_DATE</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUMBER_OF_OPENINGS</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEPARTMENT_ID</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEPARTMENT_NAME</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RECRUITER_ID</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RECRUITER</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAST_MODIFIED_DATE</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Data Type\n",
       "REQUISITION_ID               int64\n",
       "REQUISITION_UID              int64\n",
       "STATUS_IN                   object\n",
       "OPEN_DATE           datetime64[ns]\n",
       "CLOSE_DATE          datetime64[ns]\n",
       "NUMBER_OF_OPENINGS           int64\n",
       "DEPARTMENT_ID                int64\n",
       "DEPARTMENT_NAME             object\n",
       "RECRUITER_ID                 int64\n",
       "RECRUITER                   object\n",
       "LAST_MODIFIED_DATE  datetime64[ns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Specific Quality Checks:\n",
      "   - Requisitions with close date before open date: 0\n",
      "   - Requisitions with more than 10 openings: 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NUMBER_OF_OPENINGS\n",
       "1     4028\n",
       "2      605\n",
       "3      186\n",
       "4       82\n",
       "5       43\n",
       "6       24\n",
       "7        7\n",
       "8       11\n",
       "9        1\n",
       "10      13\n",
       "11       1\n",
       "12       2\n",
       "13       2\n",
       "14       2\n",
       "15       9\n",
       "18       1\n",
       "20       2\n",
       "22       1\n",
       "23       4\n",
       "30       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assessing requisitions table\n",
    "\n",
    "requisitions_quality = assess_data_quality(data['requisitions'], 'requisitions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== CANDIDATE DATA QUALITY ==============================\n",
      "\n",
      "1. Duplicates: 0 (0.00%)\n",
      "\n",
      "2. Missing Values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HISTORICAL_STATUS_END_DATE</th>\n",
       "      <td>168230</td>\n",
       "      <td>27.323061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CANDIDATE_ID</th>\n",
       "      <td>17</td>\n",
       "      <td>0.002761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Missing Values  Percentage\n",
       "HISTORICAL_STATUS_END_DATE          168230   27.323061\n",
       "CANDIDATE_ID                            17    0.002761"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Data Types:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>REQUISITION_ID</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PIPELINE_ID</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUBMISSION_DATE</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CANDIDATE_ID</th>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUBMISSION_SOURCE</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CANDIDATE_HISTORICAL_STATUS</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HISTORICAL_STATUS_START_DATE</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HISTORICAL_STATUS_END_DATE</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAST_MODIFIED_DATE</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Data Type\n",
       "REQUISITION_ID                         int64\n",
       "PIPELINE_ID                            int64\n",
       "SUBMISSION_DATE                       object\n",
       "CANDIDATE_ID                         float64\n",
       "SUBMISSION_SOURCE                     object\n",
       "CANDIDATE_HISTORICAL_STATUS           object\n",
       "HISTORICAL_STATUS_START_DATE  datetime64[ns]\n",
       "HISTORICAL_STATUS_END_DATE    datetime64[ns]\n",
       "LAST_MODIFIED_DATE            datetime64[ns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Specific Quality Checks:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Assessing candidate table\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m candidate_quality = \u001b[43massess_data_quality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcandidate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcandidate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m candidate_status_logic = check_candidate_status_logic(data[\u001b[33m'\u001b[39m\u001b[33mcandidate\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36massess_data_quality\u001b[39m\u001b[34m(df, table_name)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i+\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(date_cols)):\n\u001b[32m     48\u001b[39m     col1, col2 = date_cols[i], date_cols[j]\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     invalid_dates = df[(df[col1].notna()) & (df[col2].notna()) & (\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol1\u001b[49m\u001b[43m]\u001b[49m)]\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(invalid_dates) > \u001b[32m0\u001b[39m:\n\u001b[32m     51\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - Records with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol2\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(invalid_dates)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/aswatson/lib/python3.13/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/aswatson/lib/python3.13/site-packages/pandas/core/arraylike.py:48\u001b[39m, in \u001b[36mOpsMixin.__lt__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__lt__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__lt__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/aswatson/lib/python3.13/site-packages/pandas/core/series.py:6119\u001b[39m, in \u001b[36mSeries._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6116\u001b[39m lvalues = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m   6117\u001b[39m rvalues = extract_array(other, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6119\u001b[39m res_values = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6121\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(res_values, name=res_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/aswatson/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:347\u001b[39m, in \u001b[36mcomparison_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    344\u001b[39m     res_values = comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     res_values = \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_cmp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/aswatson/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:218\u001b[39m, in \u001b[36m_na_arithmetic_op\u001b[39m\u001b[34m(left, right, op, is_cmp)\u001b[39m\n\u001b[32m    215\u001b[39m     func = partial(expressions.evaluate, op)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    221\u001b[39m         left.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mobject\u001b[39m\n\u001b[32m    222\u001b[39m     ):\n\u001b[32m   (...)\u001b[39m\u001b[32m    225\u001b[39m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[32m    226\u001b[39m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/aswatson/lib/python3.13/site-packages/pandas/core/computation/expressions.py:242\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(op, a, b, use_numexpr)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m op_str \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[32m    241\u001b[39m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/aswatson/lib/python3.13/site-packages/pandas/core/computation/expressions.py:73\u001b[39m, in \u001b[36m_evaluate_standard\u001b[39m\u001b[34m(op, op_str, a, b)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _TEST_MODE:\n\u001b[32m     72\u001b[39m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: '<' not supported between instances of 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "# Assessing candidate table\n",
    "\n",
    "\n",
    "candidate_quality = assess_data_quality(data['candidate'], 'candidate')\n",
    "candidate_status_logic = check_candidate_status_logic(data['candidate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing candidate status table\n",
    "\n",
    "candidate_status_quality = assess_data_quality(data['candidate_status'], 'candidate_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing department table\n",
    "\n",
    "department_quality = assess_data_quality(data['department'], 'department')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aswatson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
